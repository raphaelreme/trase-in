{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from byotrack import Video, VideoTransformConfig\n",
    "\n",
    "import skimage.io as iio\n",
    "from byotrack.video.transforms import ChannelSelect, ChannelAvg, ScaleAndNormalize \n",
    "from PIL import Image\n",
    "import numpy as np #can switch this out for pytorch at somepoint - notation is identical\n",
    "\n",
    "\n",
    "icy_path = \"/home/noah/Documents/icy-2.4.2.0-all\"\n",
    "tifpath = '/home/noah/Desktop/cellsegtest/segTestNew/shortStack_adjusted' #path to sequence of tiff files\n",
    "\n",
    "#for video playback only\n",
    "fps = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reader funtion for tif sequences - handles reshaping and normalising (using the stardist recommended normaliser) (doesn't seem to perform that well)\n",
    "from csbdeep.utils import normalize as csbdeepNormaliser\n",
    "    \n",
    "def Read_Data_TIFseq(vid_path):\n",
    "    # positions = (pd.read_csv(csv_path,usecols=['TrackID','t','x','y'])).values\n",
    "    vid = iio.ImageCollection(vid_path + '/*.tif').concatenate() #concatonate to numpyarray\n",
    "    # red_vid = iio.ImageCollection(red_vid_path + '/*.tif')\n",
    "    vid = vid.reshape(vid.shape[0], vid.shape[1], vid.shape[2], 1)\n",
    "    normalisedVid = np.asarray([csbdeepNormaliser(frame) for frame in vid])\n",
    "    return normalisedVid\n",
    "\n",
    "video = Read_Data_TIFseq(tifpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the whole video with opencv\n",
    "\n",
    "for i, frame in enumerate(video):\n",
    "    try:\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.setWindowTitle('Frame', f'Frame {i} / {len(video)}')\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        key = cv2.waitKey(1000 // fps) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()\n",
    "#warning with opencv QOBject::moveToThread can be fixed: https://stackoverflow.com/questions/52337870/python-opencv-error-current-thread-is-not-the-objects-thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from byotrack.implementation.detector.stardist import StarDistDetector\n",
    "from byotrack.implementation.detector.wavelet import WaveletDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the detector object with its hyper parameters\n",
    "model_path = \"/home/noah/Desktop/STARDIST_CONFOCAL/NEWEST TdT MODELS/10X_IMAGES_ONLY\"\n",
    "detector = StarDistDetector(model_path, batch_size=5)\n",
    "#TODO: figure out batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: add roi funtionality to this so plotting shows mask over image (better evaluation) Will need to edit both stardist detector and the detector class .detect() function to include polygon data and add imageJ ROI code\n",
    "vidCopy = video[0:50] #test batch\n",
    "scale = 1\n",
    "\n",
    "global frameID\n",
    "frameID = 0\n",
    "frame = video[frameID]\n",
    "h, w = frame.shape[0:2]\n",
    "global mask_glob\n",
    "detection_zero = detector.run([vidCopy[frameID]])\n",
    "mask_glob = (detection_zero[frameID].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "\n",
    "# window_name = 'Frame', f'Frame {frameID} / {len(detections_sequence_test)} - Number of detections: {len(detections_sequence_test[i])}'\n",
    "window_name = 'Paramater Test - Segmentation   (Press Q to Quit)'\n",
    "\n",
    "try:\n",
    "\n",
    "    #create and rescale window\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(window_name, h*scale, w*scale)\n",
    "\n",
    "    #Frame Trackbar\n",
    "    def update_frame(x): #callback function for trackbar - default argument is the position of the track bar\n",
    "        detections = detector.detect(vidCopy[x][None, ...])\n",
    "        global mask_glob\n",
    "        mask_glob = (detections[0].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "    cv2.createTrackbar('Frame',window_name,0,len(vidCopy)-1,update_frame)\n",
    "\n",
    "    #Probability Trackbar\n",
    "    def update_probability_threshold(x):\n",
    "        detector.prob_threshold = (x+1)/100 \n",
    "        update_frame(frameID)\n",
    "    cv2.createTrackbar('Probability Threshold', window_name, 0, 99, update_probability_threshold)\n",
    "\n",
    "    #Overlap Trackbar\n",
    "    def update_overlap_threshold(x):\n",
    "        detector.nms_threshold = (x+1)/100\n",
    "        update_frame(frameID)\n",
    "    cv2.createTrackbar('Overlap Threshold', window_name, 0, 99, update_overlap_threshold)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "        frameID = cv2.getTrackbarPos('Frame',window_name)\n",
    "        # cv2.imshow(window_name, video[frameID])\n",
    "        cv2.imshow(window_name, mask_glob)\n",
    "\n",
    "        probabilityThreshold = cv2.getTrackbarPos('Probability Threshold', window_name)/100\n",
    "        nmsThreshold = cv2.getTrackbarPos('Overlap Threshold', window_name)/100\n",
    "\n",
    "        #exit on q\n",
    "        if cv2.waitKey(5) == ord('q'):\n",
    "            # press q to terminate the loop\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "print('Prob: ', probabilityThreshold)\n",
    "print('nms: ', nmsThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Detections\n",
    "detections_sequence = detector.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack import Track\n",
    "from byotrack.implementation.linker.icy_emht import IcyEMHTLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker = IcyEMHTLinker(icy_path)\n",
    "linker.motion = linker.Motion.BROWNIAN  # Already by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = linker.run(video, detections_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks existence in time\n",
    "\n",
    "# Transform into tensor\n",
    "tracks_tensor = Track.tensorize(tracks)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 20\n",
    "running = False\n",
    "display_detections = False\n",
    "\n",
    "frame_id = 0\n",
    "\n",
    "while True:\n",
    "    frame_id += running\n",
    "    frame = (video[frame_id] * 255).astype(np.uint8)\n",
    "    if display_detections and frame_id < len(detections_sequence):\n",
    "        mask = (detections_sequence[frame_id].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "        frame = np.concatenate((mask[..., None], frame, np.zeros_like(frame)), axis=2)\n",
    "    else:\n",
    "        frame = np.concatenate((np.zeros_like(frame), frame, np.zeros_like(frame)), axis=2)\n",
    "\n",
    "    # Add tracklets\n",
    "    for track in tracks:\n",
    "        point = track[frame_id]\n",
    "        if torch.isnan(point).any():\n",
    "            continue\n",
    "\n",
    "        x, y = point.round().to(torch.int).tolist()\n",
    "\n",
    "        color = (0, 0, 255)  # Red\n",
    "\n",
    "        cv2.circle(frame, (x, y), 5, color)\n",
    "        cv2.putText(frame, str(track.identifier % 10), (x + 4, y - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {frame_id} / {len(video)}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(1000 // fps) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\" \"):\n",
    "        running = not running\n",
    "\n",
    "    if not running and key == ord(\"w\"):  # Prev\n",
    "        frame_id = (frame_id - 1) % len(video)\n",
    "\n",
    "    if not running and key == ord(\"x\"):  # Next\n",
    "        frame_id = (frame_id + 1) % len(video)\n",
    "        \n",
    "    if key == ord(\"c\"):\n",
    "        display_detections = 1 - display_detections\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack.implementation.refiner.cleaner import Cleaner\n",
    "from byotrack.implementation.refiner.stitching import EMC2Stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = Cleaner(min_length=5, max_dist=3.5)\n",
    "tracks_clean = cleaner.run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_tensor = Track.tensorize(tracks_clean)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher = EMC2Stitcher(eta=10.0)  # Don't link tracks if they are too far (EMC dist > 5 (pixels))\n",
    "tracks_stitched = stitcher.run(video, tracks_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visualize tracks existence in time\n",
    "\n",
    "# Transform into tensor\n",
    "tracks_tensor = Track.tensorize(tracks_stitched)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 20\n",
    "running = False\n",
    "display_detections = False\n",
    "\n",
    "frame_id = 0\n",
    "\n",
    "while True:\n",
    "    frame_id += running\n",
    "    frame = (video[frame_id] * 255).astype(np.uint8)\n",
    "    if display_detections and frame_id < len(detections_sequence):\n",
    "        mask = (detections_sequence[frame_id].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "        frame = np.concatenate((mask[..., None], frame, np.zeros_like(frame)), axis=2)\n",
    "    else:\n",
    "        frame = np.concatenate((np.zeros_like(frame), frame, np.zeros_like(frame)), axis=2)\n",
    "\n",
    "    # Add tracklets\n",
    "    for track in tracks_stitched:\n",
    "        point = track[frame_id]\n",
    "        if torch.isnan(point).any():\n",
    "            continue\n",
    "\n",
    "        x, y = point.round().to(torch.int).tolist()\n",
    "\n",
    "        color = (0, 0, 255)  # Red\n",
    "\n",
    "        cv2.circle(frame, (x, y), 5, color)\n",
    "        cv2.putText(frame, str(track.identifier % 10), (x + 4, y - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {frame_id} / {len(video)}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(1000 // fps) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\" \"):\n",
    "        running = not running\n",
    "\n",
    "    if not running and key == ord(\"w\"):  # Prev\n",
    "        frame_id = (frame_id - 1) % len(video)\n",
    "\n",
    "    if not running and key == ord(\"x\"):  # Next\n",
    "        frame_id = (frame_id + 1) % len(video)\n",
    "        \n",
    "    if key == ord(\"c\"):\n",
    "        display_detections = 1 - display_detections\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tracks to dataframe and save as csv\n",
    "\n",
    "import pandas as pd\n",
    "tensorpoints = Track.tensorize(tracks_stitched)\n",
    "df = pd.DataFrame(data = tensorpoints.tolist()) #trackIndex x videoFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe for futute use\n",
    "savepath = 'testOutputs/dataframe.csv'\n",
    "df.to_csv(savepath) #or to parquet - issues with loading data lists in pandas from a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tensor as numpy array\n",
    "savepath = 'testOutputs/tracks.npy'\n",
    "array_points = tensorpoints.numpy()\n",
    "np.save(savepath, array_points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byostar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
