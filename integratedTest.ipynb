{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage.io as iio\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import byotrack\n",
    "from byotrack.implementation.detector.stardist import StarDistDetector\n",
    "from byotrack.implementation.linker.icy_emht import IcyEMHTLinker\n",
    "from byotrack.implementation.refiner.cleaner import Cleaner\n",
    "from byotrack.implementation.refiner.stitching.emc2 import EMC2Stitcher\n",
    "from byotrack.implementation.refiner.interpolater import ForwardBackwardInterpolater\n",
    "\n",
    "#icy_path = \"../pasteur/icy_build/icy.jar\"\n",
    "icy_path = \"/home/noah/Documents/icy-2.4.2.0-all/icy.jar\"\n",
    "tifpath = './ExampleData/shortStack_adjusted/' #path to sequence of tiff files\n",
    "\n",
    "#for video playback only\n",
    "fps = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsused or to be rewritten\n",
    "\n",
    "#reader funtion for tif sequences - handles reshaping and normalising (using the stardist recommended normaliser) (doesn't seem to perform that well)\n",
    "from csbdeep.utils import normalize as csbdeepNormaliser\n",
    "    \n",
    "def Read_Data_TIFseq(vid_path):\n",
    "    # positions = (pd.read_csv(csv_path,usecols=['TrackID','t','x','y'])).values\n",
    "    vid = iio.ImageCollection(vid_path + '/*.tif').concatenate() #concatonate to numpyarray\n",
    "    # red_vid = iio.ImageCollection(red_vid_path + '/*.tif')\n",
    "    vid = vid.reshape(vid.shape[0], vid.shape[1], vid.shape[2], 1)\n",
    "    #vid = np.asarray([csbdeepNormaliser(frame) for frame in vid])\n",
    "    return vid\n",
    "\n",
    "video = Read_Data_TIFseq(tifpath)\n",
    "\n",
    "# normalize video\n",
    "mini = np.quantile(video, 0.005)\n",
    "maxi = np.quantile(video, 0.999)\n",
    "\n",
    "np.clip(video, mini, maxi, video)\n",
    "video = (video - mini) / (maxi - mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TDTomato video\n",
    "\n",
    "video_path = \"../pasteur/data/long movies_tracks/6X_red_1000_2bin.avi\"\n",
    "\n",
    "video = byotrack.Video(video_path)\n",
    "\n",
    "# Test only on the 100 first frames\n",
    "video = video[:100]\n",
    "\n",
    "video.set_transform(byotrack.VideoTransformConfig(aggregate=True, normalize=True))\n",
    "\n",
    "video[0].shape, video[0].dtype, video[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the whole video with opencv\n",
    "\n",
    "for i, frame in enumerate(video):\n",
    "    try:\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.setWindowTitle('Frame', f'Frame {i} / {len(video)}')\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        key = cv2.waitKey(1000 // fps) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()\n",
    "#warning with opencv QOBject::moveToThread can be fixed: https://stackoverflow.com/questions/52337870/python-opencv-error-current-thread-is-not-the-objects-thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"../pasteur/stardist_test/models_15/111/stardist/32rays-pool2-0.0drop/lr0.0003-bs4\"\n",
    "model_path = \"/home/noah/Desktop/STARDIST_CONFOCAL/NEWEST TdT MODELS/10X_IMAGES_ONLY\"\n",
    "detector = StarDistDetector(model_path, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run this cell in order to keep the defaults prob_thresh and nms_thresh\n",
    "\n",
    "#TODO: add roi funtionality to this so plotting shows mask over image (better evaluation) Will need to edit both stardist detector and the detector class .detect() function to include polygon data and add imageJ ROI code\n",
    "vidCopy = video[0:50] #test batch\n",
    "scale = 1\n",
    "\n",
    "global frameID\n",
    "frameID = 0\n",
    "frame = video[frameID]\n",
    "h, w = frame.shape[0:2]\n",
    "global mask_glob\n",
    "detection_zero = detector.run([vidCopy[frameID]])\n",
    "mask_glob = (detection_zero[frameID].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "\n",
    "# window_name = 'Frame', f'Frame {frameID} / {len(detections_sequence_test)} - Number of detections: {len(detections_sequence_test[i])}'\n",
    "window_name = 'Paramater Test - Segmentation   (Press Q to Quit)'\n",
    "\n",
    "try:\n",
    "\n",
    "    #create and rescale window\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(window_name, h*scale, w*scale)\n",
    "\n",
    "    #Frame Trackbar\n",
    "    def update_frame(x): #callback function for trackbar - default argument is the position of the track bar\n",
    "        detections = detector.detect(vidCopy[x][None, ...])\n",
    "        global mask_glob\n",
    "        mask_glob = (detections[0].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "    cv2.createTrackbar('Frame',window_name,0,len(vidCopy)-1,update_frame)\n",
    "\n",
    "    #Probability Trackbar\n",
    "    def update_probability_threshold(x):\n",
    "        detector.prob_threshold = (x+1)/100 \n",
    "        update_frame(frameID)\n",
    "    cv2.createTrackbar('Probability Threshold', window_name, 0, 99, update_probability_threshold)\n",
    "\n",
    "    #Overlap Trackbar\n",
    "    def update_overlap_threshold(x):\n",
    "        detector.nms_threshold = (x+1)/100\n",
    "        update_frame(frameID)\n",
    "    cv2.createTrackbar('Overlap Threshold', window_name, 0, 99, update_overlap_threshold)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "        frameID = cv2.getTrackbarPos('Frame',window_name)\n",
    "        # cv2.imshow(window_name, video[frameID])\n",
    "        cv2.imshow(window_name, mask_glob)\n",
    "\n",
    "        probabilityThreshold = cv2.getTrackbarPos('Probability Threshold', window_name)/100\n",
    "        nmsThreshold = cv2.getTrackbarPos('Overlap Threshold', window_name)/100\n",
    "\n",
    "        #exit on q\n",
    "        if cv2.waitKey(5) == ord('q'):\n",
    "            # press q to terminate the loop\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "print('Prob: ', probabilityThreshold)\n",
    "print('nms: ', nmsThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Detections\n",
    "\n",
    "detections_sequence = detector.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linking\n",
    "\n",
    "linker = IcyEMHTLinker(icy_path)\n",
    "linker.motion = linker.Motion.BROWNIAN\n",
    "tracklets = linker.run(video, detections_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lifetime(tracks):\n",
    "    # Transform into tensor\n",
    "    tracks_tensor = byotrack.Track.tensorize(tracks)\n",
    "    print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "    mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "    plt.figure(figsize=(24, 16), dpi=100)\n",
    "    plt.xlabel(\"Track id\")\n",
    "    plt.ylabel(\"Frame\")\n",
    "    plt.imshow(mask)\n",
    "    plt.show()\n",
    "    \n",
    "display_lifetime(tracklets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tracks(tracks, frame_id=0, fps=20, detections_sequence=()):\n",
    "    \"\"\"Interactive visualization\n",
    "    \n",
    "    CMDS:\n",
    "        space: run/pause the video\n",
    "        x/w: Move forward/backward in time\n",
    "        c: Display detections\n",
    "    \"\"\"\n",
    "    running = False\n",
    "    display_detections = False\n",
    "\n",
    "    while True:\n",
    "        frame_id += running\n",
    "        frame = (video[frame_id] * 255).astype(np.uint8)\n",
    "        if display_detections and frame_id < len(detections_sequence):\n",
    "            mask = (detections_sequence[frame_id].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "            frame = np.concatenate((mask[..., None], frame, np.zeros_like(frame)), axis=2)\n",
    "        else:\n",
    "            frame = np.concatenate((np.zeros_like(frame), frame, np.zeros_like(frame)), axis=2)\n",
    "\n",
    "        # Add tracklets\n",
    "        for track in tracks:\n",
    "            point = track[frame_id]\n",
    "            if torch.isnan(point).any():\n",
    "                continue\n",
    "\n",
    "            i, j = point.round().to(torch.int).tolist()\n",
    "\n",
    "            color = (0, 0, 255)  # Red\n",
    "\n",
    "            cv2.circle(frame, (j, i), 5, color)\n",
    "            cv2.putText(frame, str(track.identifier % 100), (j + 4, i - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.setWindowTitle('Frame', f'Frame {frame_id} / {len(video)}')\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        key = cv2.waitKey(1000 // fps) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "            break\n",
    "\n",
    "        if key == ord(\" \"):\n",
    "            running = not running\n",
    "\n",
    "        if not running and key == ord(\"w\"):  # Prev\n",
    "            frame_id = (frame_id - 1) % len(video)\n",
    "\n",
    "        if not running and key == ord(\"x\"):  # Next\n",
    "            frame_id = (frame_id + 1) % len(video)\n",
    "\n",
    "        if key == ord(\"c\"):\n",
    "            display_detections = 1 - display_detections\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "visualize_tracks(tracklets, detections_sequence=detections_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine tracks: Clean and stich\n",
    "\n",
    "cleaner = Cleaner(min_length=5, max_dist=3.5)\n",
    "tracks_clean = cleaner.run(video, tracklets)\n",
    "\n",
    "stitcher = EMC2Stitcher(eta=5.0)  # Don't link tracks if they are too far (EMC dist > 5 (pixels))\n",
    "tracks_stitched = stitcher.run(video, tracks_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_lifetime(tracks_stitched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tracks(tracks_stitched, detections_sequence=detections_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tracks and interpolate\n",
    "\n",
    "# keep only big enough tracks (Cover at least 80% of video from start to end)\n",
    "valid_tracks = [len(t) > 0.80 * len(video) for t in tracks_stitched]\n",
    "\n",
    "interpolater = ForwardBackwardInterpolater(method=\"tps\", full = True, alpha=10.0)\n",
    "final_tracks = interpolater.run(video, tracks_stitched)  # Interpolate using all tracks, and filter afterwards\n",
    "final_tracks = [track for i, track in enumerate(final_tracks) if valid_tracks[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tracks(final_tracks, detections_sequence=detections_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byotrack.Track.save(final_tracks, \"tracks.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byostar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
