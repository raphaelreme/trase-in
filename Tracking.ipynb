{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage.io as iio\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import byotrack\n",
    "from byotrack.implementation.detector.stardist import StarDistDetector\n",
    "from byotrack.implementation.linker.icy_emht import IcyEMHTLinker\n",
    "from byotrack.implementation.refiner.cleaner import Cleaner\n",
    "from byotrack.implementation.refiner.stitching.emc2 import EMC2Stitcher\n",
    "from byotrack.implementation.refiner.interpolater import ForwardBackwardInterpolater\n",
    "from byotrack import Track\n",
    "\n",
    "icy_path = '/home/noah/Documents/icy-2.4.2.0-all/icy.jar'\n",
    "tifpath = './ExampleData/shortStack_adjusted/' #path to sequence of tiff files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Data_TIFseq(vid_path):\n",
    "    # positions = (pd.read_csv(csv_path,usecols=['TrackID','t','x','y'])).values\n",
    "    vid = iio.ImageCollection(vid_path + '/*.tif').concatenate() #concatonate to numpyarray\n",
    "    # red_vid = iio.ImageCollection(red_vid_path + '/*.tif')\n",
    "    vid = vid.reshape(vid.shape[0], vid.shape[1], vid.shape[2], 1)\n",
    "    #vid = np.asarray([csbdeepNormaliser(frame) for frame in vid])\n",
    "    return vid\n",
    "\n",
    "video = Read_Data_TIFseq(tifpath)\n",
    "\n",
    "\n",
    "# normalize video\n",
    "mini = np.quantile(video, 0.005)\n",
    "maxi = np.quantile(video, 0.999)\n",
    "\n",
    "np.clip(video, mini, maxi, video)\n",
    "video = (video - mini) / (maxi - mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in a detection sequence\n",
    "detections_sequence = np.load('/home/noah/Documents/NoahT2022/CodeRepos/Utopia/ExampleData/shortStack_adjusted/detections.npy', allow_pickle=True)\n",
    "\n",
    "#somehow this is plotting yx instead of xy!!!!!!!\n",
    "#this is probably the same issue I was having before with open cv, one of my reshaper functions is probably wrong :)\n",
    "\n",
    "# #quick fix : doesn't work in place for some reason\n",
    "# for detection_frame in detections_sequence:\n",
    "#     detection_frame.data = detection_frame.position.flip(1)\n",
    "\n",
    "# detections_sequence = [detection_frame.position.flip(1) for detection_frame in detections_sequence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(detections_sequence[0].position)\n",
    "flip = detections_sequence[0].position.flip(1)\n",
    "\n",
    "print(flip)\n",
    "# flip = np.flip(detections_sequence[0].position, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ones((2))\n",
    "arr.flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linking\n",
    "\n",
    "linker = IcyEMHTLinker(icy_path)\n",
    "linker.motion = linker.Motion.BROWNIAN\n",
    "tracklets = linker.run(video, detections_sequence) #why does the linker need the video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lifetime(tracks):\n",
    "    # Transform into tensor\n",
    "    tracks_tensor = byotrack.Track.tensorize(tracks)\n",
    "    print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "    mask =  ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "    plt.figure(figsize=(24, 16), dpi=100)\n",
    "    plt.xlabel(\"Track id\")\n",
    "    plt.ylabel(\"Frame\")\n",
    "    plt.imshow(mask)\n",
    "    plt.show()\n",
    "    \n",
    "display_lifetime(tracklets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = Cleaner(min_length=5, max_dist=3.5)\n",
    "tracks_clean = cleaner.run(video, tracklets)\n",
    "\n",
    "stitcher = EMC2Stitcher(eta=5.0)  # Don't link tracks if they are too far (EMC dist > 5 (pixels))\n",
    "tracks_stitched = stitcher.run(video, tracks_clean)\n",
    "\n",
    "display_lifetime(tracks_stitched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tracks and interpolate\n",
    "\n",
    "# keep only big enough tracks (Cover at least 80% of video from start to end)\n",
    "valid_tracks = [len(t) > 0.80 * len(video) for t in tracks_stitched]\n",
    "\n",
    "interpolater = ForwardBackwardInterpolater(method=\"tps\", full = True, alpha=10.0)\n",
    "final_tracks = interpolater.run(video, tracks_stitched)  # Interpolate using all tracks, and filter afterwards\n",
    "final_tracks = [track for i, track in enumerate(final_tracks) if valid_tracks[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set model parameters for your dataset\n",
    "\n",
    "vidCopy = video\n",
    "scale = 1\n",
    "\n",
    "global frameID\n",
    "global frame\n",
    "global frame_cv\n",
    "global contours\n",
    "\n",
    "frameID = 0\n",
    "frame = vidCopy[frameID].copy()\n",
    "frame_cv = cv2.normalize(src=frame, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "frame_cv = cv2.cvtColor(frame_cv, cv2.COLOR_GRAY2BGR)\n",
    "h, w = frame.shape[0:2]\n",
    "\n",
    "display_detections = True\n",
    "\n",
    "# window_name = 'Frame', f'Frame {frameID} / {len(detections_sequence_test)} - Number of detections: {len(detections_sequence_test[i])}'\n",
    "window_name = 'Display Tracks   (Press Q to Quit)'\n",
    "\n",
    "try:\n",
    "\n",
    "    #create and rescale window\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(window_name, h*scale, w*scale)\n",
    "\n",
    "    #Frame Trackbar\n",
    "    def update_frame(x): #callback function for trackbar - default argument is the position of the track bar\n",
    "        pass\n",
    "    cv2.createTrackbar('Frame',window_name,0,len(vidCopy)-1,update_frame)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "        frameID = cv2.getTrackbarPos('Frame',window_name)\n",
    "        frame = (vidCopy[frameID] * 255).astype(np.uint8)\n",
    "\n",
    "        if display_detections and frameID < len(final_tracks):\n",
    "            mask = (detections_sequence[frameID].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "            frame = np.concatenate((mask[..., None], frame, np.zeros_like(frame)), axis=2)\n",
    "        else:\n",
    "            frame = np.concatenate((np.zeros_like(frame), frame, np.zeros_like(frame)), axis=2)\n",
    "        \n",
    "\n",
    "        # Add tracklets\n",
    "        for track in final_tracks:\n",
    "            point = track[frameID]\n",
    "            if torch.isnan(point).any():\n",
    "                continue\n",
    "\n",
    "            i, j = point.round().to(torch.int).tolist()\n",
    "\n",
    "            color = (0, 0, 255)  # Red\n",
    "\n",
    "            cv2.circle(frame, (j, i), 5, color)\n",
    "            cv2.putText(frame, str(track.identifier % 100), (j + 4, i - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)\n",
    "\n",
    "        cv2.imshow(window_name, frame)\n",
    "\n",
    "\n",
    "\n",
    "        #exit on q\n",
    "        if cv2.waitKey(5) == ord('q'):\n",
    "            # press q to terminate the loop\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit tracks (cv mouse click functionality)\n",
    "#visualise track lifetimes interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tracks\n",
    "save_path_numpy = '/home/noah/Documents/NoahT2022/CodeRepos/Utopia/ExampleData/shortStack_adjusted/tracks'\n",
    "tensorpoints = Track.tensorize(final_tracks)\n",
    "detection_array = np.asarray(tensorpoints)\n",
    "np.save(save_path_numpy, detection_array, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byostarIJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
